---
title: "Logistic Regression. Week04"
author: "Course Notes by Fernando San Segundo"
date: "May 2015"
output: 
  html_document:
    toc: true 
---

```{r echo=FALSE, eval=FALSE}
opts_chunk$set(comment=NA, fig.width=6, fig.height=6)
```

## Introduction

These are my notes for the lectures of the [Coursera course "Introduction to Logistic Regression"](https://class.coursera.org/logisticregression-001/) by Professor Stanley Lemeshow. The goal of these notes is to provide the R code to obtain the same results as the Stata code in the lectures. Please read the *Preliminaries* of the code for lecture 1 for some details.

#### R code for previous lectures:

+ [Lecture 1.](https://rpubs.com/fernandosansegundo/82655)
+ [Lecture 2.](https://rpubs.com/fernandosansegundo/82577)
+ [Lecture 3.](https://rpubs.com/fernandosansegundo/83577)

#### <font color="red">Warning:</font> 

I have not been able to locate the ICU data set used for the last part of this week lecture (slides 19 to the end, video segments 4.5 and 4.6). That data set contains variables (e.g.  `MVENT`) that do not appear in the data set with the same name that I have. Thus, I am not able to reproduce the results in that part of the lecture. If any one can help me locate the data, I'd appreciate that.    


#### Github repository for the code:

[https://github.com/fernandosansegundo/LogisticRegressionCoursera](https://github.com/fernandosansegundo/LogisticRegressionCoursera)


## Polychotomous independent variable. Reference cell coding.

### Approaching polychotomous data via contingency tables


The contingency table in slide 2 can be entered as a matrix in R:

```{r}
CHDtable = matrix(c(5, 20, 15, 10, 20, 10, 10, 10), ncol = 2)
colnames(CHDtable) = c("PRESENT", "ABSENT")
rownames(CHDtable) = c("WHITE", "BLACK", "HISPANIC", "OTHER")
addmargins(CHDtable)
```

To generate the raw data from this table I am going to use the `expand.table` function in the `epitools` library. Uncommenting (remove \#) the first line will automatically download and install that library.
```{r}
#install.packages("epitools")
library(epitools)
CHDdf = expand.table(CHDtable)
names(CHDdf) = c("race", "chd")
head(CHDdf)
tail(CHDdf)
```

And we can check the result by converting it back to a table:

```{r}
table(CHDdf$race, CHDdf$chd)
```

This conversion to data.frame has the added benefit that `race` and `chd` have been converted to factors:
```{r}
str(CHDdf)
```
and we saw that in that case R automatically handles the dummy variables needed for logistic regression. In fact, we can use `contrasts` to ask R about the dummy coding it is going to use for a factor:
 
```{r}
contrasts(CHDdf$race)
```

As you see the *reference cell coding* is the default coding that R uses for a factor. Below we will see that `contrasts` can be used to change the dummies coding as we want.   

### Logistic model. Defining the reference factor level.  

And now we can fit the logistic model with glm:
```{r}
glmCHD = glm(chd ~ race, family = binomial(link = "logit"), data = CHDdf)
summary(glmCHD)
logLik(glmCHD)
```

You can compare the results here with those on slide 5 of the lecture pdf. 

Have you noticed something strange? The signs of the model coefficients are the opposite of the ones we obtained. Why did that happen? That's because the order of the levels in the `chd` factor is:
```{r}
levels(CHDdf$chd)
```

The culprit here is probably the `expand.table` function. At first I did not notice this because the default rule in R is to use alphabetical order for the levels of a factor. But the  `expand.table` function is doing somethng different, possibly ordering the levels in order of appearance in the data. 

Anyway, now that we know, we can easily fix this by telling R which is the reference level. We use the `relevel` function:

```{r}
CHDdf$chd = relevel(CHDdf$chd, ref = "ABSENT")
levels(CHDdf$chd)
```

In a more complex situation, if you want an arbitrary reordering of all the levels you will need to learn how to use the `levels` function. Beware however of the confussion between factor levels and the labels of those levels! 

Alternatively you can fix the contingency table itslef at the beginning, by  executing

```{r}
#(CHDtable = CHDtable[ , 2:1])
```
To do that, uncomment this line and copy it under the line `addmargins(CHDtable)`. Note, however, that this has may have undesired side effects, because many people expect the exposed group to be in the first column of the contingency table. I will just leave the table as it was.  

So, having fixed the ordering of the factor levels, the model fit now agrees with the Stata output:

```{r}
glmCHD = glm(chd ~ race, family = binomial(link = "logit"), data = CHDdf)
summary(glmCHD)
logLik(glmCHD)
```


### Odds ratio comparing each level to the reference level

Recall that our contingency table is:

```{r}
CHDtable
```

It's probably my fault, but I have not found any library function in R to compute the odds ratios from a contingency table like this one. But a nice thing about systems like R is that if you need it and you cannot find it, you can make one yourself. Bonus point: we get to use one of the R functions in the `apply` family. So, here is my function:

```{r}
oddsRatio = function(CT, refRow=1, exposedCol = 1){
   # The row vector has all the indices minus the reference group row index.  
   rows = (1: nrow(CT))[-refRow] 
   # We define a function that computes the odds-ratio of any row vs the 
   # reference row, and also labels the result. And we use sapply
   # to apply that function to each of the indices in the rows vector.
   sapply(rows, FUN = function(r){
    OR = (CT[r, 1] * CT[refRow, 2]) / (CT[r, 2] * CT[refRow, 1]) 
    names(OR) = paste0(row.names(CT)[r], " vs ", row.names(CT)[refRow], collapse = "")
    return(OR)  
   })
}
```
Note that, besides the contingency table `CT`,  the function has two additional arguments for the row index of the reference group and the column index containing the exposed group. The default values are first row and first column, but they can be set by hand. 

Using the function defaults we get: 

```{r}
oddsRatio(CHDtable)
```

but if we want to use the second row (`race = "BLACK"`) as reference, we simply do this:

```{r}
oddsRatio(CHDtable, refRow = 2)
```

Of course, an alternative way to get the odds ratio is to exponentiate the coefficients of the logistic model we fitted before:

```{r}
exp(glmCHD$coefficients)
```


### Likelihood ratio test for the significance of the `race` variable 

We have done this before (see the example of the low birth weight study in [week 2 lecture code](http://rpubs.com/fernandosansegundo/82577)), but for the sake of getting all the numerical results in the lecture I'll repeat the computation:

```{r}
(LR = glmCHD$null.deviance - glmCHD$deviance)
(df = summary(glmCHD)$df.null - summary(glmCHD)$df.residual) #degress of freedom for the chi-sq statistic
(pValue = pchisq(LR, df, lower.tail = FALSE))
```


### Confidence intervals for the odds ratios.

We already know that we can use `confint.default` to get the confidence intervals for the coefficients of the model:

```{r}
confint.default(glmCHD)
```

and exponentiate them to get confidence intervals for the odds ratios:

```{r}
exp(confint.default(glmCHD))
```
By the way, if you need a different confidence level (other than 95%), you can use an optional argument, e.g.  `level=0.99` for 99% intervals.


### Standard errors from the contingency table

Alternatively we can get the standard errors for the log odds ratios using the formula on slide 6 of the lecture notes:
\[
\hat{SE}(\ln(\hat{OR})) = \sqrt{\dfrac{1}{a} + \dfrac{1}{b} + \dfrac{1}{c} + \dfrac{1}{d} }
\]
I have repurposed for this the `oddsRatio` function that I have shown you before. Now, instead of `sapply` the function uses `lapply` to get a list containing, for each group, both the odds ratio (against the reference group) and the standard error for the odds ratio. 

 
```{r}
oddsRatio2 = function(CT, refRow=1, exposedCol = 1){
   # The row vector has all the indices minus the reference group row index.  
   rows = (1: nrow(CT))[-refRow] 
   # We define a function that computes the odds-ratio of any row vs the 
   # reference row, and also labels the result. And we use sapply
   # to apply that function to each of the indices in the rows vector.
   ORlist = lapply(rows, FUN = function(r){
    OR = (CT[r, 1] * CT[refRow, 2]) / (CT[r, 2] * CT[refRow, 1]) 
    names(OR) = paste0(row.names(CT)[r], " vs ", row.names(CT)[refRow], collapse = "")
    SE = sqrt(sum(1 / CT[c(r, refRow), ]))
    return(c(OR, SElog =SE))
   })
   return(ORlist)
}
```
In case you are wondering, the `SElog = SE` bit is just a trick to force R to use a named value and return the standard error with the name "SElog" attached. 

The result of the function is
```{r}
oddsRatio2(CHDtable)
```
You can check these values against the `Std. Error` column in the summary of the logistic model.

```{r}
summGlmCHD = summary(glmCHD)
summGlmCHD$coefficients[ ,2]
```
Either way, from here it's a small step to get the confidence intervals for the betas (remember that we have already obtained them with `confint.default`) and the odds ratios. 


## Using *deviation from means* coding.

As I told you before, we can use the `contrasts` function not only to see the dummy variables coding of a factor in R, but to change it. In this case I begin by creating the matrix for the *deviation from means* coding:

```{r}
(devMeans = rbind(rep(-1, 3), diag(rep(1, 3), nrow = 3)))
```
You can use `matrix` and directly enter the elements of the matrix, but that gets tedious with large matrices. I used `rbind` and `diag` to show you another way to do this.

Now, simply use `contrasts` to inform R that this is the coding we would like to use:

```{r}
contrasts(CHDdf$race) = devMeans
```

that's it! Now when we fit the model R uses that coding and returns the corresponding coefficients and the associated info:

```{r}
glmCHD2 = glm(chd ~ race, family = binomial(link = "logit"), data = CHDdf)
summary(glmCHD2)
logLik(glmCHD2)
```
Note that the log likelihood has not changed, it is independent of the coding.

### Variance-covariance matrix for this coding:

We get this with
```{r}
vcov(glmCHD2)
```
Be careful of the row & column ordering when comparing this to the Stata output. To make that comparison easier 
```{r}
vcov(glmCHD2)[c(2:4,1), c(2:4,1)]
```

### Interpretation of the $\beta$ coefficients in the *deviation from means* coding.

The first step is to get the odds for each of the groups. Going back to the contingency table

```{r}
CHDtable
```

the odds are obtained simply as the quotient of the columns. The log odds are immediate from that:

```{r}
(groupOdds = CHDtable[ , 1] / CHDtable[ , 2])
(groupLogOdds = log(groupOdds))
```

The average log odds is

```{r}
(meanLogOdd = mean(groupLogOdds))
```

and then subtracting from the log odds for each group

```{r}
groupLogOdds - meanLogOdd
```

Compare this with the coefficients column (labelled `Estimate`) from the model fit: 

```{r}
summary(glmCHD2)$coefficients
```

The geometric mean of the group odds is:

```{r}
prod(groupOdds)^(1/4)
```

and the odds ratio of this *average odds* with the odds for each of the groups is:

```{r}
groupOdds / prod(groupOdds)^(1/4)
```

which is the same result that you get if you exponentiate the coefficients for the groups (recall that the first beta in R is the intercept):

```{r}
betas = summary(glmCHD2)$coefficients[, 1]
exp(betas)
```

To get the odds ratio for blacks vs whites, as explained in slide 14 of the lecture, we have to do this rather non-intuitive computation:

```{r}
exp(2 * betas[2] + betas[3] + betas[4] )
```

which is the same as:

```{r}
groupOdds[2] / groupOdds[1]
```

As for the confidence intervals, notice that `confint.default` uses the same coding as the model fit. 

```{r}
confint.default(glmCHD2)
```

---

Thanks for your attention!



```{r echo=FALSE, eval=FALSE}
## Additional references (books and web sites): 

1. https://stat.ethz.ch/pipermail/r-help/2006-October/115258.html
2. http://www.inside-r.org/packages/cran/epitools/docs/expand.table
```


