---
title: "Logistic Regression. Week03"
author: "Course Notes by Fernando San Segundo"
date: "May 2015"
output: 
  html_document:
    toc: true 
---

```{r echo=FALSE, eval=FALSE}
opts_chunk$set(comment=NA, fig.width=6, fig.height=6)
```

## Introduction

These are my notes for the lectures of the [Coursera course "Introduction to Logistic Regression"](https://class.coursera.org/logisticregression-001/) by Professor Stanley Lemeshow. The goal of these notes is to provide the R code to obtain the same results as the Stata code in the lectures. Please read the *Preliminaries* of the code for lecture 1 for some details.

#### R code for previous lectures:

+ [Lecture 1.](https://rpubs.com/fernandosansegundo/82655)
+ [Lecture 2.](https://rpubs.com/fernandosansegundo/82577)

#### Github repository for the code:

[https://github.com/fernandosansegundo/LogisticRegressionCoursera](https://github.com/fernandosansegundo/LogisticRegressionCoursera)


## Variance-covariance matrix.

In the first part of the lecture we will be working with the Low Birth Weight data set, so we begin by loading it into a data.frame.

```{r}
LOWBWTdata = read.table("./data/LOWBWT.txt", header = TRUE)
```

This first section will be more theoretical in nature, for those of us who like to know some details about the maths behind the computations. So, if you are satisfied with using `vcov` in R to get the variance-covariance matrix, you can skip to the next section. If you choose to stay... you have been warned :)

In the previous lecture we used `RACE` as a predictor variable, and in my R code I converted it to a factor to let R take care of the details for us.  But for some of the work I am about to do in this section it is actually better to manually create two dummy variables `RACE2` and `RACE3` with the same meaning as the ones that we have seen in the Stata code in the lectures (where they are called  `_Irace_2` and  `_Irace_3` respectively). We will see the details below but the reason why I am doing this is because I want to have direct access to the values of these dummy variables. When we let R create them automatically, they are not added to the original data frame and that complicates the code.

Thus, let me start by creating the dummies:

```{r}
LOWBWTdata$RACE2 = as.integer(LOWBWTdata$RACE == 2)
LOWBWTdata$RACE3 = as.integer(LOWBWTdata$RACE == 3)
```

To create `RACE2` the `==` operator compares the value of `RACE` against 2 (or 3 for `RACE3`) and returns a boolean (values `TRUE` or `FALSE` in R). These booleans are converted to 0s and 1s by `as.integer`.

Now we are ready to fit the logistic model with `LWT`, `RACE2` and `RACE3` as predictors. 

```{r}
glmLOWBWT = glm(LOW ~ LWT + RACE2 + RACE3, family = binomial(link = "logit"), data = LOWBWTdata)
summary(glmLOWBWT)
```
You can check the coefficients of the resulting model against what we called the reduced model in Lecture 2 and you will see that this is the exact same model. As I said, the advantages of this direct coding of the dummies will soon become apparent. 

We have already seen that the simplest way to get the variance-covariance matrix for the model is:
```{r}
(vcovLOWBWT = vcov(glmLOWBWT))
```

Now I am going to show that this is the same matrix provided by the equation (see page 3 of the lecture pdf):
\[
\widehat{\operatorname{Var}}(\hat\beta) = [X' \hat{V} X]^{-1}
\]
where (as stated in the lecture) 
\[
X = 
\left[\begin{array}{cccc} 
1&x_{11}&\cdots&x_{1p}\\
\vdots&\vdots&\ddots&\vdots\\
1&x_{n1}&\cdots&x_{np}
\end{array}\right]
\]
and 
\[
\hat V = 
\left[
\begin{array}{cccc} 
\hat{\pi}(x_{1})\left(1 - \hat{\pi}(x_{1})\right)&0 & \cdots&0\\
0 &\hat{\pi}(x_{2})\left(1 - \hat{\pi}(x_{2})\right)&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0 &\cdots&0&\hat{\pi}(x_{n})\left(1 - \hat{\pi}(x_{n})\right)
\end{array}
\right]
\]
Finally $X'$ denotes the traspose of $X$.

Let's begin constructiong the matrix $X$. From: 

```{r}
head(LOWBWTdata)
```
and
```{r}
ncol(LOWBWTdata)
```
we see that the predictor variables are in columns 4, 12 and 13. Let's take `X0` to be that part of the data frame.

```{r}
X0 = LOWBWTdata[ , c(4, 12, 13)]
head(X0)
```
This is the part that gets complicated if you let R handle the dummies by itself. The data frame in that case does not contain columns for `RACE2` and `RACE3` and defining `X0` becomes more difficult. 

Now we face one of these data-type subtleties that give us so much fun. This `X0` is a data.frame. That's just fine when we are interested in predicting probabilities with `predict` as we did in the previous lecture:  
```{r}
piX0 = predict(glmLOWBWT, newdata = X0, type="response")
head(piX0)
```
These are the values that we are going to use to define the $\hat V$ diagonal matrix:
```{r}
V = diag(piX0 * (1 - piX0))
V[1:5, 1:5]
```

But now we need the $X$ matrix. And it has to be a matrix in R, because we are going to use it in the $X' \hat{V} X$ matrix product. A data frame won't do. Thus I begin by converting it to a matrix:

```{r}
X = as.matrix(X0)
```

and now we add the initial column of 1s to get $X$ 
```{r}
X = cbind(rep(1, nrow(X)), X)
```
the `rep`functions creates a vector with as many 1s as the number of rows of $X$ (provided by `nrow`). Then `cbind` adds that vector as the first column of `X`. 

A quick dimensionality check to ensure that the matrix product is weel defined:

```{r}
dim(X)
dim(V)
```

And we can get the estimated information matrix $\hat I(\hat\beta) = X' \hat{V} X$

```{r}
(infMatrix = t(X) %*% V %*% X )
```

With this we are ready to obtain the estimated variance-covariance matrix, as the inverse of the information matrix. In R the inverse of a non singular matrix is provided by `solve`

```{r}
(vcov_by_hand = solve(infMatrix))
```

Compare this with the matrix we obtained with `vcov`
```{r}
vcovLOWBWT
```
Up to some rounding errors, the two matrices are the same. 

### Yet another way, via likelihood.

Page 2 of the lecture pdf contains yet another description of the information matrix, and therefore another way to get the variance-covariance matrix. More precisely, the information matrix is $-Hessian(\log(\cal L))$, where $\cal L$ is the likelihood function of the model and the $Hessian$ is the matix of second order partial derivatives of the function (w.r.t. the $\beta$ parameters). The hessian is to be computed at the $\hat\beta$ estimates of the model coefficients that we obtain when we fit the model.    

The `pracma` library of R includes a `hessian` function that can be used to get a numerical approximation of the hessian. If the library is not already installed in your local R you can download it from the R repositories and install it simply by executing this command:

```{r eval=FALSE}
install.packages("pracma")
```
When the installation finishes we load the library with
```{r}
library(pracma)
```
Occasionally you will see a warning about different R versions being used in the lbrary and your system. That's almost always not a problem.

To move forward we need to define the likelihood function. Recall that 
\[
\cal L 
= \prod_{i=1}^n \pi(x_i)^{y_i}\left(1 - \pi(x_i)\right)^{1- y_i}.
\]
where 
\[
\pi(x_i) = \dfrac{e^{\beta_0 + \beta_1 x_{i1}+ \cdots + \beta_p x_{ip} }}{1 + e^{\beta_0 + \beta_1 x_{i1}+ \cdots + \beta_p x_{ip}}}
\]
(By the way, if any of these formulas look too small in your browser, right click on it and look for the Zoom options of MathJaX.)

To define the likelihood function $\cal L$ in R we begin by using $Y$ to denote the response variable (`LOW`) observed values:
```{r}
Y = LOWBWTdata$LOW
```

Now the log likelihood function is simply:
```{r}
logLikelihood = function(beta){
  logit = X %*% beta
  piX = apply(logit, MARGIN = 1, FUN = plogis)
  log(prod(piX[Y==1]) * prod(1 - piX[Y==0]))
}
```
Some remarks:

 + The first line of code in the function body uses the matrix product of `X` and `beta` to get a 1-column matrix whose $i$th row is:
  \[\beta_0 + \beta_1x_{i1}+\cdots+\beta_px_{ip}\]
 + The second line applies the logistic link function to each row of that column matrix. Thus, `piX` is again a 1-column matrix whose $i$th row is:
  \[
    \dfrac{e^{\beta_0 + \beta_1x_{i1}+\cdots+\beta_px_{ip}}}{1 + e^{\beta_0 + \beta_1x_{i1}+\cdots+\beta_px_{ip}}}  
  \]
 + The final line uses the values of $Y$ (0 or 1) to select which of the above values  are used to compute the products that give the log likelihood function.

We can check that the definition works by using our function to compute the likelihood of the fitted model: 
```{r}
logLikelihood(glmLOWBWT$coefficients)
```
and compare this result to the obtained using the built-in `logLik` function of R:
```{r}
logLik(glmLOWBWT)
```
You may be wondering why I have manually constructed `logLikelihood` when we already have a perfectly fine `logLik` function in R. The problem is that `logLik` is a function of the model (of a model object, in R parlance). And we need to have the likelihood explicitly dependent on the $\beta$ parameters, if we are to compute the hessian.  

And in fact, we are ready to do just that, computing minus the hessian of the log likelihood for the $\hat\beta$ of the fitted model:

```{r}
- hessian(logLikelihood, x0 = glmLOWBWT$coefficients)
```
And you can see that we have again arrived at the same estimated information matrix that we met before (as usual, up to some rounding):
```{r}
infMatrix
```

## Confidence interval for the logit for a single subject

Let's deal now with the confidence interval computations appearing in page 4  of the lecture pdf. The computation of a confidence interval for the logit of a single subject is similar to  what we did in Lecture 2 for the case of a single predictor variable. We will use predict, applied to a data frame containing the values of the predictor variables for the subject.     

```{r}
subjectData = data.frame(LWT=100, RACE2=1, RACE3=0)
```

Now we use predict with `type="link"` to get the answer in the logit scale and with `se.fit=TRUE` to get standard errors as part of the answer.

```{r}
(predictedLogit = predict(glmLOWBWT, newdata = subjectData, type="link", se.fit = TRUE))
```

Check these values against those appearing on page 6 of the lecture pdf. The confidence interval for the logit is now easily obtained with:

```{r}
(intervalLogit = predictedLogit$fit + c(-1, 1) *  (qnorm(0.975) * predictedLogit$se.fit))
```

And to get the interval for the probability we use the `plogis` function (this is the logistic link function, as we saw in the R code for the previous lecture):
 
```{r}
(intervalProb = plogis(intervalLogit))
```
If you want the predicted probability use:
```{r}
(predictedProb = predict(glmLOWBWT, newdata = subjectData, type="response"))
```


## Interpretation of coefficients for dichotomous independent variable in terms of the odds ratio 

In this part of the lecture we will be working with the CHD data set:

```{r}
CHDdata = read.table("./data/CHDAGE.txt", header = TRUE)
```

We are going to define a dichotomous variable called `AGE55` which is equal to 1 if the subject's age is equal to or less than 55, and 0 otherwise. We add the new variable to the data frame:

```{r}
CHDdata$AGE55 = as.integer(CHDdata$AGE >= 55) 
head(CHDdata)
tail(CHDdata)
```

Let's reproduce the contingency table on page 19 of the lecture pdf.   

```{r}
(table1 = addmargins(table(CHDdata$CHD, CHDdata$AGE55)[2:1,2:1]))
```

I have had to fiddle a bit with the result of `table`, to get the variables in the same order as in the lecture. The `addmargins` function does just that: add the marginal totals to a table.

Our first estimate of the odds ratio can be obtained with the well-known formula from the diagonals of the contingency table:
```{r}
(oddsRatio1 = table1[1,1] * table1[2,2] / (table1[1,2] * table1[2,1]))
```

Let's fit a logistic model to the data:
```{r}
glmCHD = glm(CHD ~ AGE55, family = binomial(link = "logit"), CHDdata)
(summGlmCHD = summary(glmCHD))
```
to see that the estimated odds ratio equals the exponential of the model coefficient for the dichotomous predictor variable:
 
```{r}
(oddsRatio2 = exp(glmCHD$coefficients[2]))
```

### Confidence interval for the odds ratio

We begin by computing 
\[
\widehat{\operatorname{Var}}(\hat\beta_1) = \left(\dfrac{1}{a} + \dfrac{1}{b} + \dfrac{1}{c}  + \dfrac{1}{d} \right)
\]

```{r}
(varBeta1 = sum(1/table1[1:2, 1:2]))
```
and from this the standard error is:

```{r}
(SEb1 = sqrt(varBeta1))
```
Therefore the confidence interval for $\beta_1$ is:

```{r}
(confint_beta1 = glmCHD$coefficients[2] + c(-1, 1) * qnorm(0.975) * SEb1)
```
and exponentiating this we get the confidence interval for the odds ratio:

```{r}
(confintOddsRatio = exp(confint_beta1))
```
Alternatively we can turn to `confint.default` to get the confidence interval for $\beta_1$ (and $\beta_0$)

```{r}
(confint_beta =  confint.default(level = 0.95, glmCHD))
confint_beta[2,]
```
The confidence interval for $\beta_1$ is the second row (use `[2, ]` to select it).

---

Thanks for your attention!



```{r echo=FALSE, eval=FALSE}
## Additional references (books and web sites): 

1. http://sites.stat.psu.edu/~jls/stat544/syllabus.html

```


